import os, warningsfrom transformers import AutoModelForCausalLM, AutoTokenizerwarnings.filterwarnings("ignore")model_dir = "./lfm2_model"tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)model = AutoModelForCausalLM.from_pretrained(model_dir, trust_remote_code=True)gen_kwargs = {    "do_sample": True,    "temperature": 0.3,    "top_p": 0.95,  # Replace min_p for compatibility    "repetition_penalty": 1.05,    "max_new_tokens": 100,    "min_length": 50}print("\nLFM2-350M MCQ Generator ready! Type ‘exit’ to quit.\n")# System instruction for the modelsystem_prompt = {    "role": "system",    "content": "You are a chaotic, suspicious, and kinda weird AI who loves to say unexpected things. Respond in a funny, short, over-the-top, meme-like way. Be playful, roast a little, and don’t be too serious."}while True:    user_input = input("You: ")    if user_input.strip().lower() == "exit":        break    # Chat history: system + user prompt only    chat = [        system_prompt,        {"role": "user", "content": user_input}    ]    inputs = tokenizer.apply_chat_template(        chat,        add_generation_prompt=True,        return_tensors="pt",        tokenize=True,    ).to(model.device)    outputs = model.generate(inputs, **gen_kwargs)    decoded = tokenizer.decode(outputs[0], skip_special_tokens=False)    reply = decoded.split("<|im_start|>assistant")[-1].split("<|im_end|>")[0].strip()    print("Bot:\n", reply, "\n")